---
title: 'Assignment #6'
author: "Elliot Smith"
date: "10/1/2018"
output: pdf_document
---

```{r, echo = FALSE}

########## Workspace Prep ##########



## Load in the necessaey packages

library(coda)

```

# Problem 1

## Part i

$p(\beta|y, \lambda_1, \lambda_2, k) \propto p(y|\lambda_1, \lambda_2, k, \beta) \times p(\lambda_1|y, \lambda_2, k, \beta) \times p(\lambda_2|y, \lambda_1, k, \beta) \times p(k|y, \lambda_1, \lambda_2, \beta) \times p(\beta)$
\newline
\newline
Since $p(y|\lambda_1, \lambda_2, k, \beta)$ and $p(k|y, \lambda_1, \lambda_2, \beta)$ don't depend on $\beta$, we may ignore them and our equation becomes:
\newline
\newline
$p(\beta|y, \lambda_1, \lambda_2, k) \propto p(\lambda_1|y, \lambda_2, k, \beta) \times p(\lambda_2|y, \lambda_1, k, \beta) \times p(\beta)$
\newline
\newline
Where:

\[
\begin{aligned}
p(\lambda_1|y, \lambda_2, k, \beta) & = p(y|\lambda_1, \lambda_2, k)p(\lambda_1) \\
& \sim Gamma\bigg(\alpha + \sum_{t=1}^k y_t, \beta + k\bigg)
\end{aligned}
\]

\[
\begin{aligned}
p(\lambda_2|y, \lambda_1, k, \beta) & = p(y|\lambda_1, \lambda_2, k)p(\lambda_2) \\
& \sim Gamma\bigg(\alpha + \sum_{t=k+1}^n y_t, \beta + (n - k)\bigg)
\end{aligned}
\]

So:

\[
\begin{aligned}
p(\beta|y, \lambda_1, \lambda_2, k) & \propto p(\lambda_1|y, \lambda_2, k, \beta) \times p(\lambda_2|y, \lambda_1, k, \beta) \times p(\beta) \\
& \propto \beta^{\alpha}\lambda_1^{\alpha - 1 + \sum_{t=1}^k y_t}e^{-(\beta + k)\lambda_1}\beta^{\alpha}
\lambda_2^{\alpha - 1 + \sum_{t=k+1}^n y_t}e^{-(\beta + (n - k))\lambda_2}\beta^{a_\beta - 1}e^{-{b_\beta}\beta} \\
& \propto \beta^{2\alpha}e^{-(\beta + k)\lambda_1 - (\beta + (n - k))\lambda_2}\beta^{a_\beta - 1}e^{-{b_\beta}\beta} \\
& \propto \beta^{2\alpha + a_\beta - 1}e^{-\beta\lambda_1 - \beta\lambda_2 - b_\beta{\beta}} \\
& \propto \beta^{2\alpha + a_\beta - 1}e^{-(\lambda_1 + \lambda_2 + b_\beta)\beta} \\
& \sim Gamma(2\alpha + a_\beta, \lambda_1 + \lambda_2 + b_\beta)
\end{aligned}
\]

## Part ii

Please refer to the Code Appendix section for the implementation of the Gibbs sampler.

```{r, echo = FALSE}

##### Part ii #####



### Unchanged code from Lecture



year = (1851:1962)
y = c(4, 5, 4, 1, 0, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6, 3, 3, 5, 4, 5, 3,
      1, 4, 4, 1, 5, 5, 3, 4, 2, 5, 2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1,
      1, 1, 3, 0, 0, 1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,
      0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2, 3, 3, 1, 1, 2,
      1, 1, 1, 1, 2, 4, 2, 0, 0, 0, 1, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0,
      1, 0, 0, 1, 0, 1)

n = length(y)

# Hyperparameters:
alpha = 0.5; beta = 0.01

# Pick some initial values:
k = ceiling(n/2) # midpoint is CP
lambda_1 = mean(y[1:k])
lambda_2 = mean(y[(k+1):n])

# Sample from the posterior distribution:
S = 10^4

# Storage:
post_lambda_1 = array(0, c(S, 1))
post_lambda_2 = array(0, c(S, 1))
post_k = array(0, c(S, 1));

for(s in 1:S){
    # Sample lambda_1:
    lambda_1 = rgamma(n = 1,
                      shape = alpha + sum(y[1:k]),
                      rate = beta + k)
    # Sample lambda_2:
    # NOTE: assuming k < n throughout!
    lambda_2 = rgamma(n = 1,
                      shape = alpha + sum(y[(k+1):n]),
                      rate = beta + (n-k))
    # Sample k:
    log_g = cumsum(y)*log(lambda_1/lambda_2) + (1:n)*(lambda_2 - lambda_1)
    #pm<-exp(log_g)/sum(exp(log_g)); k<-min((1:n)[runif(1)<cumsum(pm)])
    k = sample(1:n, 1, prob = exp(log_g))
    
    # Store:
    post_lambda_1[s] = lambda_1
    post_lambda_2[s] = lambda_2
    post_k[s] = k
}



### Edited code from lecture, factors in beta prior



# Hyperparameters:
alpha = 0.5; beta = 0.01

## Set the beta params

b_alpha <- 0.1
b_beta <- 0.1

# Pick some initial values:
k = ceiling(n/2) # midpoint is CP
lambda_1 = mean(y[1:k])
lambda_2 = mean(y[(k+1):n])

# Sample from the posterior distribution:
S = 10^4

# Storage:
post_lambda_1_new = array(0, c(S, 1))
post_lambda_2_new = array(0, c(S, 1))
post_k_new = array(0, c(S, 1))
post_beta_new = array(0, c(S, 1))

for(s in 1:S){
    
    # Sample lambda_1:
    lambda_1 = rgamma(n = 1,
                      shape = alpha + sum(y[1:k]),
                      rate = beta + k)
    # Sample lambda_2:
    # NOTE: assuming k < n throughout!
    lambda_2 = rgamma(n = 1,
                      shape = alpha + sum(y[(k+1):n]),
                      rate = beta + (n-k))
    # Sample k:
    log_g = cumsum(y)*log(lambda_1/lambda_2) + (1:n)*(lambda_2 - lambda_1)
    #pm<-exp(log_g)/sum(exp(log_g)); k<-min((1:n)[runif(1)<cumsum(pm)])
    k = sample(1:n, 1, prob = exp(log_g))
    
    # Sample beta:
    
    beta <- rgamma(n = 1,
                   shape = (2 * alpha) + b_alpha,
                   rate = lambda_1 + lambda_2 + b_beta)
    
    # Store:
    post_lambda_1_new[s] = lambda_1
    post_lambda_2_new[s] = lambda_2
    post_k_new[s] = k
    post_beta_new[s] = beta
}

```

## Part iii

```{r, echo = FALSE}

##### Part iii #####



## Compute the HPD intervals for beta = 0.01

ci_lambda_1 <- as.vector(HPDinterval(as.mcmc(post_lambda_1), prob = 0.95)[1, 1:2])
ci_lambda_2 <- as.vector(HPDinterval(as.mcmc(post_lambda_2), prob = 0.95)[1, 1:2])
ci_k = as.vector(HPDinterval(as.mcmc(post_k + 1850), prob = 0.95)[1, 1:2])

cat("HPD Interval Lambda_1 with Beta = 0.01: ", "(", ci_lambda_1[1], ",", ci_lambda_1[2], ")")
cat("HPD Interval Lambda_2 with Beta = 0.01: ", "(", ci_lambda_2[1], ",", ci_lambda_2[2], ")")
cat("HPD Interval k + 1850 with Beta = 0.01: ", "(", ci_k[1], ",", ci_k[2], ")")

## Compute the HPD intervals for beta hyperprior

ci_lambda_1_new <- as.vector(HPDinterval(as.mcmc(post_lambda_1_new), prob = 0.95)[1, 1:2])
ci_lambda_2_new <- as.vector(HPDinterval(as.mcmc(post_lambda_2_new), prob = 0.95)[1, 1:2])
ci_k_new = as.vector(HPDinterval(as.mcmc(post_k_new + 1850), prob = 0.95)[1, 1:2])

cat("HPD Interval Lambda_1 with Beta Hyperprior: ", "(", ci_lambda_1_new[1], ",", ci_lambda_1_new[2], ")")
cat("HPD Interval Lambda_2 with Beta Hyperprior: ", "(", ci_lambda_2_new[1], ",", ci_lambda_2_new[2], ")")
cat("HPD Interval k + 1850 with Beta Hyperprior: ", "(", ci_k_new[1], ",", ci_k_new[2], ")")

```

There does not appear to be substantial difference between the posterior credible intervals of the two models.

## Part iv

```{r, echo = FALSE}

##### Part iv #####



par(mfrow = c(1, 1))

hist(post_beta_new, freq = FALSE)
x <- rgamma(n = S, shape = b_alpha, rate = b_beta)
lines(sort(x), y = dgamma(sort(x), shape = b_alpha, rate = b_beta), lwd = 2)
abline(v = 0.01, lwd = 5)

```

No. According to the results and the density representation there is not evidence to suggest the the posterior distribution $[\beta|y]$ differs substantially from our previous choice of $\beta = 0.01$

# Problem 2

## Part i

\[
\begin{aligned}
p(y|\mu, \alpha, \beta) & = \int \frac{\beta^\alpha}{\Gamma(\alpha)}\tau^{\alpha - 1}e^{-\beta\tau}\bigg(\frac{\tau}{2\pi}\bigg)^{\frac{1}{2}}e^{-\frac{\tau}{2}(y - \mu)^2}d\tau \\
& = \frac{\beta^\alpha}{\Gamma(\alpha)}\frac{1}{(2\pi)^{\frac{1}{2}}} \int \tau^{\alpha - \frac{1}{2}}e^{-\beta\tau}e^{-\frac{\tau}{2}(y - \mu)^2}d\tau \\
& = \frac{\beta^\alpha}{\Gamma(\alpha)}\frac{1}{(2\pi)^{\frac{1}{2}}} \int \tau^{\alpha - \frac{1}{2}}e^{-(\beta + \frac{1}{2}(y - \mu)^2)\tau}d\tau \\
& = \frac{\beta^\alpha}{\Gamma(\alpha)}\frac{1}{(2\pi)^{\frac{1}{2}}}\frac{\Gamma(\alpha + \frac{1}{2})}{(\beta + \frac{1}{2}(y - \mu)^2)^{\alpha + \frac{1}{2}}} \\
& = \frac{\Gamma(\alpha + \frac{1}{2})}{\Gamma(\alpha)}\frac{1}{(2\pi\beta)^{\frac{1}{2}}}\frac{1}{(1 + \frac{1}{2\beta}(y - \mu)^2)^{\alpha + \frac{1}{2}}}
\end{aligned}
\]

And now substituting in for $z_i$ and our Gamma parameters for $\alpha$ and $\beta$ we get the following result:
\newline
\newline
$p(z_i) = \frac{\Gamma(\frac{\nu + 1}{2})}{\sqrt{\pi\nu}\Gamma(\frac{\nu}{2})}\frac{1}{\bigg(1 + \frac{z_i^2}{\nu}\bigg)^{\frac{\nu + 1}{2}}}$

## Part ii

```{r, echo = FALSE}

##### Part ii #####



nu <- 4
S <- 10000

post_gamma <- rgamma(n = S, shape = (nu / 2), rate = (nu / 2)) 

post_norm <- rnorm(n = S, mean = 0, sd = 1 / post_gamma)

t_dens <- rt(n = S, df = nu)

par(mfrow = c(1, 2))

hist(post_norm, xlim = c(-25, 25), breaks = 200)
hist(t_dens, xlim = c(-25, 25), breaks = 20)

```

As we can see from our result, we get similar density when comparing our results from Part i with the $t_{\nu}(0,1)$ distribution.

## Part iii

Yes, this result does make sense, because whenever the variance of a normally distributed random variable is unknown and a conjugate prior placed over it that follows a Gamma distribution, the resulting marginal distribution of the variable will follow a Student's t-distribution.

# Problem 3

## Part i

\[
\begin{aligned}
p(\theta|y, \tau_y, \xi_i) & \propto p(\theta)p(y|\theta, \tau_y, \xi_i)  \\
\end{aligned}
\]

\[
\begin{aligned}
p(\tau_y|y, \theta, \xi_i) & \propto p(\tau_y)p(y|\theta, \tau_y, \xi_i)  \\
\end{aligned}
\]

\[
\begin{aligned}
p(\xi_i|y, \theta, \tau_y) & \propto p(\xi_i)p(y|\theta, \tau_y, \xi_i)  \\
\end{aligned}
\]

## Part ii

## Part iii

## Part iv

## Part v

# Code Appendix












